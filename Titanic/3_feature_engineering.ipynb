{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e923a2fb",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a083267",
   "metadata": {},
   "source": [
    "# 3. Feature Engineering\n",
    "Feature engineering is the process of selecting, manipulating, and transforming raw data into features that can be used in supervised learning. Feature engineering is one of the most important steps when it comes to a machine learning project because algorithms can only interpret data in a certain way (numerical). For example, categorical data must be binary encoded for it to be considered during the fitting process. Furthermore, there could be features that disrupt the correlation between the label and other features, so removing them is just as important. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e817a924",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libraries. \n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import os\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "\n",
    "# Importing our functions. \n",
    "import functions\n",
    "import plot_functions\n",
    "\n",
    "# Importing chosen models. \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "# Importing transformers. \n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, OneHotEncoder\n",
    "\n",
    "# Converts plotly output into static images, so it can be viewed inside github repository. \n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"svg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1a748c9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Embarked</th>\n",
       "      <th>Age Group</th>\n",
       "      <th>Martial Status</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Name</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Braund, Mr. Owen Harris</th>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>young adult</td>\n",
       "      <td>Not Married</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cumings, Mrs. John Bradley (Florence Briggs Thayer)</th>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>middle aged</td>\n",
       "      <td>Married</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    Pclass     Sex   Age  \\\n",
       "Name                                                                       \n",
       "Braund, Mr. Owen Harris                                  3    male  22.0   \n",
       "Cumings, Mrs. John Bradley (Florence Briggs Tha...       1  female  38.0   \n",
       "\n",
       "                                                    SibSp  Parch     Ticket  \\\n",
       "Name                                                                          \n",
       "Braund, Mr. Owen Harris                                 1      0  A/5 21171   \n",
       "Cumings, Mrs. John Bradley (Florence Briggs Tha...      1      0   PC 17599   \n",
       "\n",
       "                                                       Fare Embarked  \\\n",
       "Name                                                                   \n",
       "Braund, Mr. Owen Harris                              7.2500        S   \n",
       "Cumings, Mrs. John Bradley (Florence Briggs Tha...  71.2833        C   \n",
       "\n",
       "                                                      Age Group  \\\n",
       "Name                                                              \n",
       "Braund, Mr. Owen Harris                             young adult   \n",
       "Cumings, Mrs. John Bradley (Florence Briggs Tha...  middle aged   \n",
       "\n",
       "                                                   Martial Status  Survived  \n",
       "Name                                                                         \n",
       "Braund, Mr. Owen Harris                               Not Married         0  \n",
       "Cumings, Mrs. John Bradley (Florence Briggs Tha...        Married         1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root = os.getcwd() # Setting root directory as cwd. \n",
    "df = pd.read_csv(r\"{}\\train_new_features.csv\".format(root)) # Importing the train set.\n",
    "df.set_index(\"Name\", inplace=True)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca42577",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd5c640",
   "metadata": {},
   "source": [
    "### Section A: Data Preparation\n",
    "In this section we'll focus on standardizing and normalizing our numerical dataset to hopefully boost performance from our chosen algorithms. However, before we implement these transforms, we must first set our benchmarks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a47a26fe",
   "metadata": {},
   "source": [
    "**A.1: Benchmarks** <br>\n",
    "The first step is to set up our benchmarks, so we have a point of reference against which things may be compared. This is a very important step whenever conducting empirical testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ef763e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model_evaluation() class sets up the pipeline to transform the data then returns a cross validated score. \n",
    "LR = functions.model_evaluation()\n",
    "LR.preprocessing(df)\n",
    "LR.add_pipe_component(\"clf\", LogisticRegression(max_iter=1000))\n",
    "LR.cross_validation(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6e80dd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "GBC = functions.model_evaluation()\n",
    "GBC.preprocessing(df)\n",
    "GBC.add_pipe_component(\"clf\", GradientBoostingClassifier())\n",
    "GBC.cross_validation(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c2f24d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB = functions.model_evaluation()\n",
    "XGB.preprocessing(df)\n",
    "XGB.add_pipe_component(\"clf\", XGBClassifier())\n",
    "XGB.cross_validation(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fde940b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up benchmarks. \n",
    "benchmarkLR = {\"Base_LR\" : LR.cv_result}\n",
    "benchmarkGBC = {\"Base_GBC\" : GBC.cv_result}\n",
    "benchmarkXGB = {\"Base_XGB\" : XGB.cv_result}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c48aa754",
   "metadata": {},
   "source": [
    "| Model | Model Type | Benchmark |  \n",
    "|---------|---------|---------|\n",
    "| **Logistic Regression** | Base Model | 81.64% |\n",
    "| **Gradient Boosting Classifier** | Base Model | 81.45% |\n",
    "| **XGB Classifier** | Base Model | 81.42% |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edee10db",
   "metadata": {},
   "source": [
    "**A.2: Standardization** <br>\n",
    "Standardization converts the mean to 0 and a standard deviation of 1. Usually this transform is conducted on Gaussian distributions. Standardizing your dataset can potentially lead you to an uplift in performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "769fddbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We reject the null hypothesis with the new benchmark for LR_Standardized: 82.5760%\n"
     ]
    }
   ],
   "source": [
    "# The model_evaluation() class sets up the pipeline to transform the data then returns a cross validated score. \n",
    "LR_s = functions.model_evaluation()\n",
    "LR_s.preprocessing(df)\n",
    "LR_s.add_pipe_component(\"s\", StandardScaler())\n",
    "LR_s.add_pipe_component(\"clf\", LogisticRegression(max_iter=1000))\n",
    "LR_s.cross_validation(df)\n",
    "\n",
    "# Our hypothesis is that a standardized dataset will boost model performance.  \n",
    "functions.hypothesis_testing(LR_s.cv_result, benchmarkLR, \"LR_Standardized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "059eb94f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We reject the null hypothesis with the new benchmark for GBC_standardized: 81.6383%\n"
     ]
    }
   ],
   "source": [
    "# The model_evaluation() class sets up the pipeline to transform the data then returns a cross validated score. \n",
    "GBC_s = functions.model_evaluation()\n",
    "GBC_s.preprocessing(df)\n",
    "GBC_s.add_pipe_component(\"s\", StandardScaler())\n",
    "GBC_s.add_pipe_component(\"clf\", GradientBoostingClassifier())\n",
    "GBC_s.cross_validation(df)\n",
    "\n",
    "functions.hypothesis_testing(GBC_s.cv_result, benchmarkGBC, \"GBC_standardized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03ae7d63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We accept the null hypothesis.\n"
     ]
    }
   ],
   "source": [
    "# The model_evaluation() class sets up the pipeline to transform the data then returns a cross validated score. \n",
    "XGB_s = functions.model_evaluation()\n",
    "XGB_s.preprocessing(df)\n",
    "XGB_s.add_pipe_component(\"s\", StandardScaler())\n",
    "XGB_s.add_pipe_component(\"clf\", XGBClassifier())\n",
    "XGB_s.cross_validation(df)\n",
    "\n",
    "functions.hypothesis_testing(XGB_s.cv_result, benchmarkXGB, \"XGB_standardized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab306a89",
   "metadata": {},
   "source": [
    "| Model | Model Type | Benchmark |  \n",
    "|---------|---------|---------|\n",
    "| **Logistic Regression** | Standardized | 82.58% |\n",
    "| **Gradient Boosting Classifier** | Standardized | 81.69% |\n",
    "| **XGB Classifier** | Base Model | 81.42% |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7164b63d",
   "metadata": {},
   "source": [
    "**A.3: Normalization** <br>\n",
    "Normalization is the process of rescaling the dataset to the range of 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0258ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We accept the null hypothesis.\n"
     ]
    }
   ],
   "source": [
    "# The model_evaluation() class sets up the pipeline to transform the data then returns a cross validated score. \n",
    "LR_n = functions.model_evaluation()\n",
    "LR_n.preprocessing(df)\n",
    "LR_n.add_pipe_component(\"n\", Normalizer())\n",
    "LR_n.add_pipe_component(\"clf\", LogisticRegression(max_iter=1000))\n",
    "LR_n.cross_validation(df)\n",
    "\n",
    "# Our hypothesis is that a normalized dataset will boost model performance.  \n",
    "functions.hypothesis_testing(LR_n.cv_result, benchmarkLR, \"LR_normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f1dacef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We accept the null hypothesis.\n"
     ]
    }
   ],
   "source": [
    "# The model_evaluation() class sets up the pipeline to transform the data then returns a cross validated score. \n",
    "GBC_n = functions.model_evaluation()\n",
    "GBC_n.preprocessing(df)\n",
    "GBC_n.add_pipe_component(\"n\", Normalizer())\n",
    "GBC_n.add_pipe_component(\"clf\", GradientBoostingClassifier())\n",
    "GBC_n.cross_validation(df)\n",
    "\n",
    "functions.hypothesis_testing(GBC_n.cv_result, benchmarkGBC, \"GBC_normalized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fcdbfe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We accept the null hypothesis.\n"
     ]
    }
   ],
   "source": [
    "# The model_evaluation() class sets up the pipeline to transform the data then returns a cross validated score. \n",
    "XGB_n = functions.model_evaluation()\n",
    "XGB_n.preprocessing(df)\n",
    "XGB_n.add_pipe_component(\"n\", Normalizer())\n",
    "XGB_n.add_pipe_component(\"clf\", XGBClassifier())\n",
    "XGB_n.cross_validation(df)\n",
    "\n",
    "functions.hypothesis_testing(XGB_n.cv_result, benchmarkXGB, \"XGB_ntandardized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b56fec9",
   "metadata": {},
   "source": [
    "| Model | Model Type | Benchmark |  \n",
    "|---------|---------|---------|\n",
    "| **Logistic Regression** | Standardized | 82.58% |\n",
    "| **Gradient Boosting Classifier** | Standardized | 81.69% |\n",
    "| **XGB Classifier** | Base Model | 81.42% |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa9fafd",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf957ba4",
   "metadata": {},
   "source": [
    "### Section B: Recursive Feature Elimination\n",
    "In this section we'll focus be focusing on Feature Elimination using scikit-learn's Recrusive Feature Elimination algorithm. RFE is a feature elimination technique which aims to find the most important features when it comes to predicting our labels. It works through our entire feature space, recursively considering smaller and smaller sets of features. Through each iteration it assigns weights to features (e.g., the feature_importance_ attribute in any classification estimator) and uses this metric to gauge how imporant each feature is at predicting the target variable. At the end of the process, only the most important features are kept. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62038cee",
   "metadata": {},
   "source": [
    "**B.1: Transforming our Data** <br>\n",
    "However, before we can conduct RFE, we must first transform our feature space so that it's suitable to be fed into the algorithm (i.e., impute missing values, encode categorical features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ba88baac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical columns are: ['Age', 'Fare']\n",
      "Categorical columns are: ['Pclass', 'Sex', 'SibSp', 'Parch', 'Ticket', 'Embarked', 'Age Group', 'Martial Status']\n"
     ]
    }
   ],
   "source": [
    "# Separating our numerical & categorical columns for ColumnTransformer. \n",
    "numerical = list(df.select_dtypes(['float64']).columns)\n",
    "print(f'Numerical columns are: {numerical}')\n",
    "\n",
    "categorical = list(df.select_dtypes(exclude=['float64']).columns)\n",
    "categorical.remove(\"Survived\") # Remove label from data. \n",
    "print(f'Categorical columns are: {categorical}')\n",
    "\n",
    "cat_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder(handle_unknown='ignore', sparse=False))\n",
    "])\n",
    "\n",
    "# Define numerical pipeline\n",
    "num_pipe = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='median'))\n",
    "])\n",
    "\n",
    "# Combine categorical and numerical pipelines\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('cat', cat_pipe, categorical),\n",
    "    ('num', num_pipe, numerical)\n",
    "])\n",
    "\n",
    "# Fit a pipeline with transformers and an estimator to the training data\n",
    "pipe = Pipeline([\n",
    "    ('preprocessor', preprocessor)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "09f2df1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass_1</th>\n",
       "      <th>Pclass_2</th>\n",
       "      <th>Pclass_3</th>\n",
       "      <th>Sex_female</th>\n",
       "      <th>Sex_male</th>\n",
       "      <th>SibSp_0</th>\n",
       "      <th>SibSp_1</th>\n",
       "      <th>SibSp_2</th>\n",
       "      <th>SibSp_3</th>\n",
       "      <th>SibSp_4</th>\n",
       "      <th>...</th>\n",
       "      <th>Embarked_S</th>\n",
       "      <th>Age Group_child</th>\n",
       "      <th>Age Group_middle aged</th>\n",
       "      <th>Age Group_senior</th>\n",
       "      <th>Age Group_young adult</th>\n",
       "      <th>Martial Status_Married</th>\n",
       "      <th>Martial Status_Not Married</th>\n",
       "      <th>Age</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 712 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Pclass_1  Pclass_2  Pclass_3  Sex_female  Sex_male  SibSp_0  SibSp_1  \\\n",
       "0       0.0       0.0       1.0         0.0       1.0      0.0      1.0   \n",
       "1       1.0       0.0       0.0         1.0       0.0      0.0      1.0   \n",
       "\n",
       "   SibSp_2  SibSp_3  SibSp_4  ...  Embarked_S  Age Group_child  \\\n",
       "0      0.0      0.0      0.0  ...         1.0              0.0   \n",
       "1      0.0      0.0      0.0  ...         0.0              0.0   \n",
       "\n",
       "   Age Group_middle aged  Age Group_senior  Age Group_young adult  \\\n",
       "0                    0.0               0.0                    1.0   \n",
       "1                    1.0               0.0                    0.0   \n",
       "\n",
       "   Martial Status_Married  Martial Status_Not Married   Age     Fare  Survived  \n",
       "0                     0.0                         1.0  22.0   7.2500         0  \n",
       "1                     1.0                         0.0  38.0  71.2833         1  \n",
       "\n",
       "[2 rows x 712 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract SimpleImputer transfrom from the num_pipe.  \n",
    "num_data = pipe.named_steps['preprocessor'].transformers[1][1].fit_transform(df[numerical])\n",
    "# Creating dataframe. \n",
    "num_df = pd.DataFrame(data=num_data, columns=list(df[numerical].columns))\n",
    "\n",
    "# Imputing the categorical data. \n",
    "imputed_data = pipe.named_steps['preprocessor'].transformers[0][1][0].fit_transform(df[categorical])\n",
    "imputed_cat =  pd.DataFrame(imputed_data, columns=categorical)\n",
    "\n",
    "# Encoding the categorical data. \n",
    "encoded_data = pipe.named_steps['preprocessor'].transformers[0][1][1].fit_transform(imputed_cat)\n",
    "cat_df = pd.DataFrame(encoded_data, columns=pipe.named_steps['preprocessor'].transformers[0][1][1].\n",
    "                      get_feature_names_out(categorical))\n",
    "\n",
    "# Merging both numerical and categorical features back into a single dataframe. \n",
    "merged = pd.concat([cat_df, num_df], axis=1) \n",
    "df.reset_index(inplace=True)\n",
    "df = df.drop(columns=[\"Name\"])\n",
    "processed = pd.concat([merged, df.iloc[:,-1]], axis=1) # Attaching our dependant variable to our processed dataset. \n",
    "processed.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ecfe168",
   "metadata": {},
   "source": [
    "**B.2: Logistic Regression**<br>\n",
    "Now let's implement RFECV onto our base Logistic Regression Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d4b9e697",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of features: 590\n"
     ]
    }
   ],
   "source": [
    "LR_rfe = functions.model_evaluation()\n",
    "# arguments(data, model) \n",
    "LR_rfe.RFE_cross_validate(processed, LogisticRegression(max_iter=1000))\n",
    "print(\"Optimal number of features: %d\" % LR_rfe.rfe_result.n_features_)\n",
    "\n",
    "# Store results into a dictionary. \n",
    "dictLR = LR_rfe.rfe_result.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7024e0b",
   "metadata": {},
   "source": [
    "**B.3: Gradient Boosting Classifier**<br>\n",
    "Now let's implement RFECV onto our base Gradient Boosting Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "46554324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of features: 6\n"
     ]
    }
   ],
   "source": [
    "GBC_rfe = functions.model_evaluation()\n",
    "# arguments(data, model) \n",
    "GBC_rfe.RFE_cross_validate(processed, GradientBoostingClassifier())\n",
    "print(\"Optimal number of features: %d\" % GBC_rfe.rfe_result.n_features_)\n",
    "\n",
    "# Store results into a dictionary. \n",
    "dictGBC = GBC_rfe.rfe_result.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c55832",
   "metadata": {},
   "source": [
    "**B.4: XGB Classifier**<br>\n",
    "Now let's implement RFECV onto our base XGB Classifier Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "080b59a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal number of features: 23\n"
     ]
    }
   ],
   "source": [
    "XGB_rfe = functions.model_evaluation()\n",
    "# arguments(data, model) \n",
    "XGB_rfe.RFE_cross_validate(processed, XGBClassifier())\n",
    "print(\"Optimal number of features: %d\" % XGB_rfe.rfe_result.n_features_)\n",
    "\n",
    "# Store results into a dictionary. \n",
    "dictXGB = XGB_rfe.rfe_result.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "33d68c8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg class=\"main-svg\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"980\" height=\"450\" style=\"\" viewBox=\"0 0 980 450\"><rect x=\"0\" y=\"0\" width=\"980\" height=\"450\" style=\"fill: rgb(255, 255, 255); fill-opacity: 1;\"/><defs id=\"defs-9f7620\"><g class=\"clips\"><clipPath id=\"clip9f7620xyplot\" class=\"plotclip\"><rect width=\"665\" height=\"270\"/></clipPath><clipPath class=\"axesclip\" id=\"clip9f7620x\"><rect x=\"80\" y=\"0\" width=\"665\" height=\"450\"/></clipPath><clipPath class=\"axesclip\" id=\"clip9f7620y\"><rect x=\"0\" y=\"100\" width=\"980\" height=\"270\"/></clipPath><clipPath class=\"axesclip\" id=\"clip9f7620xy\"><rect x=\"80\" y=\"100\" width=\"665\" height=\"270\"/></clipPath></g><g class=\"gradients\"/><g class=\"patterns\"/></defs><g class=\"bglayer\"><rect class=\"bg\" x=\"80\" y=\"100\" width=\"665\" height=\"270\" style=\"fill: rgb(248, 248, 246); fill-opacity: 1; stroke-width: 0;\"/></g><g class=\"layer-below\"><g class=\"imagelayer\"/><g class=\"shapelayer\"/></g><g class=\"cartesianlayer\"><g class=\"subplot xy\"><g class=\"layer-subplot\"><g class=\"shapelayer\"/><g class=\"imagelayer\"/></g><g class=\"minor-gridlayer\"><g class=\"x\"/><g class=\"y\"/></g><g class=\"gridlayer\"><g class=\"x\"><path class=\"xgrid crisp\" transform=\"translate(172.73000000000002,0)\" d=\"M0,100v270\" style=\"stroke: rgb(255, 255, 255); stroke-opacity: 1; stroke-width: 1px;\"/><path class=\"xgrid crisp\" transform=\"translate(266.39,0)\" d=\"M0,100v270\" style=\"stroke: rgb(255, 255, 255); stroke-opacity: 1; stroke-width: 1px;\"/><path class=\"xgrid crisp\" transform=\"translate(360.05,0)\" d=\"M0,100v270\" style=\"stroke: rgb(255, 255, 255); stroke-opacity: 1; stroke-width: 1px;\"/><path class=\"xgrid crisp\" transform=\"translate(453.71,0)\" d=\"M0,100v270\" style=\"stroke: rgb(255, 255, 255); stroke-opacity: 1; stroke-width: 1px;\"/><path class=\"xgrid crisp\" transform=\"translate(547.37,0)\" d=\"M0,100v270\" style=\"stroke: rgb(255, 255, 255); stroke-opacity: 1; stroke-width: 1px;\"/><path class=\"xgrid crisp\" transform=\"translate(641.04,0)\" d=\"M0,100v270\" style=\"stroke: rgb(255, 255, 255); stroke-opacity: 1; stroke-width: 1px;\"/><path class=\"xgrid crisp\" transform=\"translate(734.7,0)\" d=\"M0,100v270\" style=\"stroke: rgb(255, 255, 255); stroke-opacity: 1; stroke-width: 1px;\"/></g><g class=\"y\"><path class=\"ygrid crisp\" transform=\"translate(0,341.72)\" d=\"M80,0h665\" style=\"stroke: rgb(255, 255, 255); stroke-opacity: 1; stroke-width: 1px;\"/><path class=\"ygrid crisp\" transform=\"translate(0,281.34000000000003)\" d=\"M80,0h665\" style=\"stroke: rgb(255, 255, 255); stroke-opacity: 1; stroke-width: 1px;\"/><path class=\"ygrid crisp\" transform=\"translate(0,220.97)\" d=\"M80,0h665\" style=\"stroke: rgb(255, 255, 255); stroke-opacity: 1; stroke-width: 1px;\"/><path class=\"ygrid crisp\" transform=\"translate(0,160.59)\" d=\"M80,0h665\" style=\"stroke: rgb(255, 255, 255); stroke-opacity: 1; stroke-width: 1px;\"/></g></g><g class=\"zerolinelayer\"/><path class=\"xlines-below\"/><path class=\"ylines-below\"/><g class=\"overlines-below\"/><g class=\"xaxislayer-below\"/><g class=\"yaxislayer-below\"/><g class=\"overaxes-below\"/><g class=\"plot\" transform=\"translate(80,100)\" clip-path=\"url(#clip9f7620xyplot)\"><g class=\"scatterlayer mlayer\"><g class=\"trace scatter trace8f1740\" style=\"stroke-miterlimit: 2; opacity: 1;\"><g class=\"fills\"/><g class=\"errorbars\"/><g class=\"lines\"><path class=\"js-line\" d=\"M0,167.69L0.94,167.69L1.87,166.59L2.81,164.44L3.75,164.44L4.68,164.44L5.62,158.06L6.56,158.06L7.49,160.86L8.43,161.19L9.37,138.86L12.18,128.11L13.11,131.76L14.05,132.78L14.99,132.5L15.92,129.31L16.86,129.82L17.8,140.84L18.73,140.84L19.67,137.59L20.61,137.59L30.91,137.59L31.85,130.11L35.59,130.11L36.53,126.92L37.46,126.92L38.4,138.18L39.34,138.18L40.27,128.02L41.21,124.83L42.15,124.83L46.83,124.83L47.77,118.37L48.7,115.19L49.64,115.19L51.51,115.19L52.45,118.45L53.39,115.26L54.32,115.26L58.07,115.26L59.01,113.16L66.5,113.16L67.44,109.53L69.31,109.53L70.25,112.11L71.18,112.11L72.12,108.55L73.06,108.55L73.99,105.36L74.93,105.36L75.87,133.62L79.61,134.24L80.55,124.46L84.3,124.46L85.23,120.37L87.11,120.37L88.04,114.05L92.73,114.05L93.66,101.96L97.41,101.96L98.35,105.08L102.09,105.08L103.03,108.28L104.9,108.28L105.84,112.36L107.71,101.88L108.65,105.97L109.58,109.6L110.52,105.97L113.33,105.97L114.27,109.17L115.2,105.97L116.14,105.97L125.51,105.97L126.44,99.59L127.38,92.72L128.32,92.72L129.25,92.72L130.19,89.04L133.94,89.04L134.87,85.88L144.24,85.88L145.18,99.08L146.11,95.89L147.05,95.89L160.16,95.89L161.1,91.83L162.97,91.33L163.91,91.33L170.46,91.33L171.4,95.09L172.34,95.09L173.27,91.83L179.83,91.83L180.77,95.89L191.07,95.89L192.01,92.13L195.75,92.13L196.69,98.15L197.63,92.13L198.56,92.13L199.5,88.4L200.44,92.13L217.3,92.13L218.23,95.32L222.92,95.32L223.85,97.27L224.79,101.34L225.73,101.34L226.66,101.34L227.6,97.27L228.54,97.27L229.47,93.54L230.41,93.95L231.35,97.69L235.09,97.7L236.03,100.89L236.96,100.89L237.9,91.24L238.84,100.89L239.77,100.89L246.33,100.89L247.27,96.64L248.2,93.44L249.14,93.44L250.08,93.44L251.01,96.64L251.95,96.64L252.89,87.01L254.76,87.01L255.7,78.89L258.51,78.89L259.44,75.68L270.68,75.68L271.62,78.89L297.85,78.89L298.78,75.7L392.44,75.7L393.38,72.5L544.18,72.5L545.11,75.3L550.73,75.3L551.67,67.95L552.61,71.61L553.54,67.95L554.48,67.95L555.42,71.42L556.35,75.08L557.29,68.22L558.23,75.08L559.16,75.08L563.85,75.08L564.78,71.34L569.46,71.34L570.4,74.97L571.34,71.34L572.27,71.34L588.2,71.34L589.13,74.97L590.07,71.34L591.01,71.34L665,71.34\" style=\"vector-effect: non-scaling-stroke; fill: none; stroke: rgb(99, 110, 250); stroke-opacity: 1; stroke-width: 2px; opacity: 1;\"/></g><g class=\"points\"/><g class=\"text\"/></g><g class=\"trace scatter tracee1f706\" style=\"stroke-miterlimit: 2; opacity: 1;\"><g class=\"fills\"/><g class=\"errorbars\"/><g class=\"lines\"><path class=\"js-line\" d=\"M0,167.69L0.94,149.87L1.87,161.87L3.75,42.76L4.68,13.5L5.62,44.97L6.56,34.78L7.49,64.13L8.43,51.88L10.3,40.39L11.24,43.03L12.18,48.77L13.11,74.45L14.99,35.02L15.92,77.45L16.86,56.02L17.8,60.61L18.73,48.96L19.67,61.14L20.61,52.11L21.54,54.8L22.48,58.79L23.42,46.26L24.35,52.4L25.29,68.41L26.23,45.33L27.16,63.86L28.1,56.67L29.04,63.22L29.97,59.67L30.91,64.16L31.85,65.47L32.78,62.85L33.72,49.94L34.65,56.76L36.53,64.9L37.46,64.39L38.4,72L39.34,58.95L40.27,62.73L41.21,72.47L42.15,55.17L43.08,59.6L44.02,70.36L44.96,70.01L45.89,80.32L46.83,73.23L47.77,82.14L48.7,70.92L49.64,82.79L50.58,74.31L51.51,86.69L52.45,78.08L53.39,78.05L54.32,80.26L55.26,75.1L56.2,80.4L58.07,77.46L59.01,74.86L59.94,74.31L60.88,75.79L61.82,79.65L62.75,74.95L63.69,74.72L64.63,68.29L65.56,82.87L66.5,74.12L67.44,65.81L68.37,81.79L69.31,78.37L70.25,81.58L71.18,80.94L72.12,89.19L73.06,82.62L73.99,70.12L75.87,85.24L76.8,82.32L77.74,82.32L78.68,85.53L79.61,82.11L80.55,82.11L81.49,85.53L82.42,85.97L83.36,78.46L84.3,78.42L85.23,78.92L86.17,78.46L87.11,78.46L88.04,73.67L89.92,81.58L90.85,80.94L92.73,74.72L93.66,82.32L94.6,81.58L95.54,78.46L98.35,76.88L99.28,78.46L100.22,77.42L101.15,70.82L102.09,80.63L103.03,82.32L103.96,77.72L104.9,80.95L105.84,77.84L106.77,85.53L107.71,77.86L108.65,82.08L110.52,82.32L111.46,85.97L112.39,84.9L113.33,78.46L114.27,78.37L115.2,80.95L116.14,85.29L117.08,82.32L118.01,78.46L118.95,88.45L119.89,82.11L120.82,84.8L121.76,82.32L122.7,76.33L123.63,81.68L124.57,77.72L125.51,69.89L126.44,72.69L127.38,64.99L128.32,78.58L130.19,82.32L131.13,74.56L132.06,74.41L133,77.38L133.94,78.92L134.87,84.8L135.81,81.58L136.75,85.34L138.62,74.72L139.56,81.58L140.49,85.34L141.43,77.72L142.37,85.97L143.3,78.46L144.24,73.67L145.18,73.67L147.05,85.97L147.99,74.56L148.92,82.32L149.86,77.72L151.73,85.97L152.67,81.68L153.61,81.58L154.54,82.87L155.48,81.58L156.42,74.72L157.35,81.58L158.29,74.41L159.23,69.81L160.16,78.46L161.1,85.53L162.04,85.97L162.97,69.81L163.91,82.32L165.78,72.95L166.72,69.89L167.65,73.46L168.59,85.53L169.53,78.46L170.46,78.46L171.4,82.32L172.34,77.09L173.27,77.84L174.21,82.32L175.15,89.19L176.08,85.24L177.02,73.67L177.96,78.42L178.89,84.8L179.83,81.37L180.77,85.78L181.7,81.68L182.64,89.19L183.58,80.95L184.51,85.24L185.45,85.97L186.39,88.45L187.32,78.46L188.26,78.42L189.2,82.11L190.13,78.42L191.07,78.42L192.01,78.46L192.94,70.55L193.88,78.59L194.82,78.46L195.75,84.8L196.69,74.56L197.63,85.53L198.56,85.53L199.5,82.32L200.44,85.17L201.37,75.79L202.31,74.56L203.25,85.97L204.18,78.46L205.12,85.97L206.06,82.32L206.99,77.39L207.93,82.11L208.87,72.9L209.8,81.79L210.74,89.19L211.68,81.68L212.61,77.44L213.55,81.58L214.49,84.8L215.42,82.11L217.3,88.68L218.23,78.46L220.11,84.6L221.04,77.72L221.98,73.67L222.92,82.08L223.85,88.64L224.79,78.42L225.73,85.53L226.66,74.56L227.6,84.8L228.54,82.87L229.47,85.53L230.41,78.42L232.28,85.97L233.22,78.46L234.15,80.95L235.09,77.62L236.96,85.53L237.9,70.55L238.84,74.41L239.77,85.97L240.71,74.41L241.65,85.53L242.58,69.89L243.52,77.94L244.46,82.87L245.39,74.56L246.33,69.95L247.27,88.55L248.2,81.68L249.14,81.68L251.01,82.32L251.95,85.24L252.89,78.46L253.82,89.19L254.76,81.58L255.7,81.58L256.63,70.55L257.57,73L258.51,74.2L259.44,82.32L260.38,61.92L261.32,82.32L262.25,66.65L263.19,78.22L264.13,74.72L265.06,79.02L266,77.2L266.94,78.46L267.87,81.68L268.81,71.9L269.75,79.65L270.68,73.64L271.62,84.9L272.56,76.88L273.49,85.24L274.43,85.97L275.37,73.98L276.3,85.76L277.24,73.77L278.18,82.23L279.11,73.72L280.05,85.24L280.99,84.8L281.92,78.46L282.86,82.32L283.8,81.58L284.73,77.62L285.67,88.45L287.54,82.08L288.48,82.32L289.42,78.46L290.35,81.68L291.29,85.53L292.23,81.58L294.1,82.32L295.04,77.84L295.97,81.68L296.91,82.32L297.85,78.28L298.78,80.95L299.72,82.32L300.65,82.32L301.59,77.72L302.53,80.95L303.46,82.32L304.4,77.86L305.34,80.95L306.27,78.46L308.15,85.54L309.08,77.79L310.02,79.65L310.96,78.58L311.89,82.32L312.83,82.32L314.7,74.56L315.64,74.04L316.58,65.09L317.51,81.58L318.45,82.23L319.39,78.46L320.32,73.98L321.26,85.53L322.2,78.46L323.13,78.46L324.07,74.56L325.01,61.86L325.94,96.06L326.88,81.58L327.82,82.32L328.75,78.58L329.69,71.05L330.63,85.53L331.56,73.2L332.5,85.53L333.44,74.56L334.37,88.45L335.31,89.19L336.25,85.97L337.18,78.46L338.12,77.72L339.99,85.53L340.93,82.32L341.87,82.32L342.8,88.68L343.74,69.81L344.68,82.32L345.61,77.73L346.55,80.95L347.49,81.68L348.42,84.8L349.36,85.97L350.3,85.53L351.23,78.46L352.17,81.64L354.04,89.65L354.98,85.54L355.92,77.77L356.85,82.32L357.79,78.42L358.73,85.97L359.66,81.49L360.6,82.32L361.54,78.46L362.47,70.55L363.41,84.9L364.35,74.41L366.22,78.46L367.15,81.68L368.09,85.53L369.03,84.97L369.96,74.41L370.9,78.46L372.77,82.32L373.71,81.68L374.65,84.8L375.58,78.46L376.52,82.32L377.46,78.46L378.39,78.58L379.33,81.8L380.27,81.68L381.2,75.79L382.14,82.32L383.08,82.11L384.01,77.3L384.95,78.46L385.89,73.98L386.82,74.2L387.76,74.41L388.7,85.47L389.63,88.74L390.57,84.8L391.51,74.72L392.44,78.46L393.38,85.53L394.32,81.68L395.25,81.64L396.19,74.41L397.13,88.74L398.06,82.11L399,81.58L399.94,78.37L400.87,81.68L401.81,75.79L402.75,74.41L403.68,82.32L404.62,77.84L405.56,89.21L406.49,78.46L407.43,78.46L408.37,81.68L409.3,82.08L411.18,82.32L412.11,84.8L413.99,78.42L414.92,78.42L417.73,78.42L418.67,74.72L419.61,82.32L420.54,77.72L421.48,78.42L422.42,81.68L423.35,78.58L424.29,78.46L426.16,82.11L427.1,78.46L428.04,81.68L428.97,78.46L431.78,88.45L432.72,82.11L435.53,73.67L436.46,78.46L437.4,85.53L438.34,66.72L441.15,85.36L442.08,78.42L443.02,82.11L443.96,77.79L445.83,79.65L446.77,78.46L447.7,77.94L448.64,66L449.58,77.72L450.51,75.76L451.45,82.32L452.39,82.11L453.32,88.45L454.26,82.32L455.2,85.53L456.13,81.58L457.07,77.84L458.01,78.42L458.94,82.32L459.88,78.42L460.82,88L461.75,77.72L462.69,66.65L463.63,72.79L464.56,85.97L465.5,82.32L466.44,81.37L467.37,78.46L469.25,85.97L470.18,85.97L471.12,85.97L472.06,74.72L473.93,81.58L474.87,77.72L476.74,78.46L477.68,81.68L478.61,86L479.55,72L481.42,81.68L482.36,78.46L483.3,82.32L484.23,81.58L485.17,70.55L486.11,74.56L487.98,78.58L488.92,78.42L489.85,77.72L490.79,78.58L491.73,77.72L492.66,78.46L494.54,82.32L495.47,78.46L497.35,81.68L498.28,77.84L499.22,82.11L500.15,78.46L501.09,82.32L502.03,74.56L503.9,82.11L504.84,71.9L505.77,82.32L506.71,78.22L507.65,82.32L508.58,78.46L509.52,78.37L510.46,82.32L511.39,81.58L512.33,84.8L514.2,74.41L515.14,89.65L516.08,74.56L517.01,82.11L517.95,77.32L518.89,83.31L520.76,70.55L521.7,85.97L522.63,82.11L523.57,81.58L525.44,77.72L526.38,82.32L527.32,74.41L528.25,79.65L529.19,82.32L530.13,81.79L531.06,82.32L532,70.55L532.94,85.53L533.87,85.53L535.75,78.92L536.68,77.72L537.62,82.32L538.56,77.79L539.49,70.3L540.43,85.34L541.37,81.58L542.3,81.05L543.24,79.41L544.18,88.48L545.11,89.19L546.05,81.27L546.99,73.92L547.92,85.54L548.86,80.95L549.8,80.95L550.73,73.72L551.67,77.62L552.61,77.78L553.54,85.53L557.29,86.09L558.23,82.87L559.16,80.95L560.1,81.58L561.04,85.53L561.97,81.68L562.91,85.29L563.85,85.29L564.78,81.37L565.72,84.8L566.65,81.68L567.59,81.68L568.53,88.45L569.46,76.88L570.4,82.32L571.34,78.07L572.27,77.79L573.21,80.95L574.15,85.53L575.08,85.34L576.96,76.85L577.89,77.72L578.83,81.68L579.77,77.73L580.7,84.8L581.64,80.85L582.58,77.79L583.51,88.45L584.45,81.44L585.39,81.68L586.32,78.37L587.26,70.89L588.2,77.72L589.13,77.89L590.07,78.42L591.01,77.2L591.94,73.98L592.88,85.97L593.82,80.94L594.75,82.11L596.63,81.68L597.56,82.23L598.5,85.97L599.44,81.68L600.37,74.72L601.31,81.68L602.25,73.22L603.18,81.79L604.12,81.79L605.06,73.77L605.99,78.58L606.93,73.96L607.87,70.7L608.8,82.57L609.74,79.45L610.68,82.32L611.61,70.51L612.55,81.68L613.49,78.58L614.42,78.37L616.3,85.53L617.23,81.68L618.17,85.53L619.11,78.58L620.04,82.32L620.98,70.82L621.92,85.34L622.85,76.25L623.79,81.68L624.73,73.67L625.66,66.89L626.6,77.79L627.54,89.19L628.47,80.95L629.41,74.72L630.35,81.68L631.28,85.53L632.22,81.68L634.09,74.63L635.03,84.8L636.9,85.34L637.84,81.68L638.77,84.25L639.71,73.77L640.65,85.53L641.58,82.19L643.46,81.68L644.39,77.79L645.33,77.42L646.27,81.68L647.2,81.64L648.14,74.04L649.08,74.56L650.01,77.72L650.95,81.64L651.89,77.61L652.82,78.46L653.76,76.75L654.7,73.77L655.63,77.42L657.51,78.37L658.44,81.64L659.38,77.57L660.32,85.47L661.25,84.17L662.19,74.56L663.13,82.23L664.06,77.84L665,81.58\" style=\"vector-effect: non-scaling-stroke; fill: none; stroke: rgb(239, 85, 59); stroke-opacity: 1; stroke-width: 2px; opacity: 1;\"/></g><g class=\"points\"/><g class=\"text\"/></g><g class=\"trace scatter tracebbd189\" style=\"stroke-miterlimit: 2; opacity: 1;\"><g class=\"fills\"/><g class=\"errorbars\"/><g class=\"lines\"><path class=\"js-line\" d=\"M0,167.69L0.94,256.5L1.87,237.2L2.81,221.9L3.75,163.71L5.62,119.1L6.56,85.31L8.43,108.05L9.37,77.92L10.3,98.45L11.24,78.69L12.18,87.44L13.11,85.52L14.05,82.47L14.99,62.13L15.92,76.12L16.86,68.4L17.8,68.08L18.73,78.46L19.67,58.41L20.61,58.4L665,58.4\" style=\"vector-effect: non-scaling-stroke; fill: none; stroke: rgb(0, 204, 150); stroke-opacity: 1; stroke-width: 2px; opacity: 1;\"/></g><g class=\"points\"/><g class=\"text\"/></g></g></g><g class=\"overplot\"/><path class=\"xlines-above crisp\" d=\"M0,0\" style=\"fill: none;\"/><path class=\"ylines-above crisp\" d=\"M0,0\" style=\"fill: none;\"/><g class=\"overlines-above\"/><g class=\"xaxislayer-above\"><g class=\"xtick\"><text text-anchor=\"middle\" x=\"0\" y=\"383\" transform=\"translate(172.73000000000002,0)\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre; opacity: 1;\">100</text></g><g class=\"xtick\"><text text-anchor=\"middle\" x=\"0\" y=\"383\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre; opacity: 1;\" transform=\"translate(266.39,0)\">200</text></g><g class=\"xtick\"><text text-anchor=\"middle\" x=\"0\" y=\"383\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre; opacity: 1;\" transform=\"translate(360.05,0)\">300</text></g><g class=\"xtick\"><text text-anchor=\"middle\" x=\"0\" y=\"383\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre; opacity: 1;\" transform=\"translate(453.71,0)\">400</text></g><g class=\"xtick\"><text text-anchor=\"middle\" x=\"0\" y=\"383\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre; opacity: 1;\" transform=\"translate(547.37,0)\">500</text></g><g class=\"xtick\"><text text-anchor=\"middle\" x=\"0\" y=\"383\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre; opacity: 1;\" transform=\"translate(641.04,0)\">600</text></g><g class=\"xtick\"><text text-anchor=\"middle\" x=\"0\" y=\"383\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre; opacity: 1;\" transform=\"translate(734.7,0)\">700</text></g></g><g class=\"yaxislayer-above\"><g class=\"ytick\"><text text-anchor=\"end\" x=\"79\" y=\"4.199999999999999\" transform=\"translate(0,341.72)\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre; opacity: 1;\">0.76</text></g><g class=\"ytick\"><text text-anchor=\"end\" x=\"79\" y=\"4.199999999999999\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre; opacity: 1;\" transform=\"translate(0,281.34000000000003)\">0.78</text></g><g class=\"ytick\"><text text-anchor=\"end\" x=\"79\" y=\"4.199999999999999\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre; opacity: 1;\" transform=\"translate(0,220.97)\">0.8</text></g><g class=\"ytick\"><text text-anchor=\"end\" x=\"79\" y=\"4.199999999999999\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre; opacity: 1;\" transform=\"translate(0,160.59)\">0.82</text></g><g class=\"ytick\"><text text-anchor=\"end\" x=\"79\" y=\"4.199999999999999\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre; opacity: 1;\" transform=\"translate(0,100.22)\">0.84</text></g></g><g class=\"overaxes-above\"/></g></g><g class=\"polarlayer\"/><g class=\"smithlayer\"/><g class=\"ternarylayer\"/><g class=\"geolayer\"/><g class=\"funnelarealayer\"/><g class=\"pielayer\"/><g class=\"iciclelayer\"/><g class=\"treemaplayer\"/><g class=\"sunburstlayer\"/><g class=\"glimages\"/><defs id=\"topdefs-9f7620\"><g class=\"clips\"/><clipPath id=\"legend9f7620\"><rect width=\"210\" height=\"67\" x=\"0\" y=\"0\"/></clipPath></defs><g class=\"layer-above\"><g class=\"imagelayer\"/><g class=\"shapelayer\"/></g><g class=\"infolayer\"><g class=\"legend\" pointer-events=\"all\" transform=\"translate(758.3000000000001,100)\"><rect class=\"bg\" shape-rendering=\"crispEdges\" style=\"stroke: rgb(68, 68, 68); stroke-opacity: 1; fill: rgb(255, 255, 255); fill-opacity: 1; stroke-width: 0px;\" width=\"210\" height=\"67\" x=\"0\" y=\"0\"/><g class=\"scrollbox\" transform=\"\" clip-path=\"url(#legend9f7620)\"><g class=\"groups\"><g class=\"traces\" transform=\"translate(0,14.5)\" style=\"opacity: 1;\"><text class=\"legendtext\" text-anchor=\"start\" x=\"40\" y=\"4.680000000000001\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre;\">Logistic Regression</text><g class=\"layers\" style=\"opacity: 1;\"><g class=\"legendfill\"/><g class=\"legendlines\"><path class=\"js-line\" d=\"M5,0h30\" style=\"fill: none; stroke: rgb(99, 110, 250); stroke-opacity: 1; stroke-width: 2px;\"/></g><g class=\"legendsymbols\"><g class=\"legendpoints\"/></g></g><rect class=\"legendtoggle\" x=\"0\" y=\"-9.5\" width=\"204.28125\" height=\"19\" style=\"fill: rgb(0, 0, 0); fill-opacity: 0;\"/></g><g class=\"traces\" transform=\"translate(0,33.5)\" style=\"opacity: 1;\"><text class=\"legendtext\" text-anchor=\"start\" x=\"40\" y=\"4.680000000000001\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre;\">Gradient Boosting Classifer</text><g class=\"layers\" style=\"opacity: 1;\"><g class=\"legendfill\"/><g class=\"legendlines\"><path class=\"js-line\" d=\"M5,0h30\" style=\"fill: none; stroke: rgb(239, 85, 59); stroke-opacity: 1; stroke-width: 2px;\"/></g><g class=\"legendsymbols\"><g class=\"legendpoints\"/></g></g><rect class=\"legendtoggle\" x=\"0\" y=\"-9.5\" width=\"204.28125\" height=\"19\" style=\"fill: rgb(0, 0, 0); fill-opacity: 0;\"/></g><g class=\"traces\" transform=\"translate(0,52.5)\" style=\"opacity: 1;\"><text class=\"legendtext\" text-anchor=\"start\" x=\"40\" y=\"4.680000000000001\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 12px; fill: rgb(42, 63, 95); fill-opacity: 1; white-space: pre;\">XGB Classifier</text><g class=\"layers\" style=\"opacity: 1;\"><g class=\"legendfill\"/><g class=\"legendlines\"><path class=\"js-line\" d=\"M5,0h30\" style=\"fill: none; stroke: rgb(0, 204, 150); stroke-opacity: 1; stroke-width: 2px;\"/></g><g class=\"legendsymbols\"><g class=\"legendpoints\"/></g></g><rect class=\"legendtoggle\" x=\"0\" y=\"-9.5\" width=\"204.28125\" height=\"19\" style=\"fill: rgb(0, 0, 0); fill-opacity: 0;\"/></g></g></g><rect class=\"scrollbar\" rx=\"20\" ry=\"3\" width=\"0\" height=\"0\" style=\"fill: rgb(128, 139, 164); fill-opacity: 1;\" x=\"0\" y=\"0\"/></g><g class=\"g-gtitle\"><text class=\"gtitle\" x=\"49\" y=\"50\" text-anchor=\"start\" dy=\"0em\" style=\"font-family: 'Arial Black'; font-size: 20px; fill: rgb(42, 63, 95); opacity: 1; font-weight: normal; white-space: pre;\">Recursive Feature Elimination for Selected Models</text></g><g class=\"g-xtitle\"><text class=\"xtitle\" x=\"412.5\" y=\"410.8\" text-anchor=\"middle\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 14px; fill: rgb(42, 63, 95); opacity: 1; font-weight: normal; white-space: pre;\">Total Features Selected</text></g><g class=\"g-ytitle\"><text class=\"ytitle\" transform=\"rotate(-90,26.934375000000003,235)\" x=\"26.934375000000003\" y=\"235\" text-anchor=\"middle\" style=\"font-family: 'Open Sans', verdana, arial, sans-serif; font-size: 14px; fill: rgb(42, 63, 95); opacity: 1; font-weight: normal; white-space: pre;\">Weighted f1 score</text></g></g></svg>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Setting up axes values. \n",
    "x_axis = list(range(1, len(dictLR['mean_test_score']) + 1)) # Maximum no. of features is 711.\n",
    "y_LR = dictLR['mean_test_score']\n",
    "y_GBC = dictGBC['mean_test_score']\n",
    "y_XGB = dictXGB['mean_test_score']\n",
    "\n",
    "plot_functions.plot_rfe(xaxis=x_axis, yLR=y_LR, yGBC=y_GBC, yXGB=y_XGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a09c850",
   "metadata": {},
   "source": [
    "* In conclusion we saw minor improvements across all models with the RFE technique.\n",
    "* The small boost in performance does not justify the long training hours it takes to eliminate redundnat features for only a 1-2% boost in performance.\n",
    "* Moving forward, we will not be including RFE into our pipeline, for efficiency purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c52560",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a487543",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "**Author:** Aaron Chew&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **|** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Date Published:** 11/08/2022 &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**|** &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**Email:** aaronsamuelchew@gmail.com &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;**|**&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; **GitHub:** https://github.com/aaron-chew  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
